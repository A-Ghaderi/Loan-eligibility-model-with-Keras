{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature information ##\n",
    "## This function provides description for variables in dataset ##\n",
    "import pandas as pd\n",
    "data_info = pd.read_csv('lending_club_info.csv',index_col='LoanStatNew')\n",
    "print(data_info.loc['revol_util']['Description'])\n",
    "\n",
    "def feat_info(col_name):\n",
    "    print(data_info.loc[col_name]['Description'])\n",
    "    \n",
    "feat_info('mort_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries and data ##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# might be needed depending on your version of Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('lending_club_loan_two.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Countplot of prediction variable ##\n",
    "sns.countplot(x='loan_status',data=df)\n",
    "\n",
    "## Histogram of loan amount ##\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.distplot(df['loan_amnt'],kde=False,bins=40)\n",
    "plt.xlim(0,45000)\n",
    "\n",
    "## Correlation of variable and their heatmap ##\n",
    "df.corr()\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(df.corr(),annot=True,cmap='viridis')\n",
    "plt.ylim(10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation between loan_amnt and installment #\n",
    "feat_info('installment')\n",
    "feat_info('loan_amnt')\n",
    "\n",
    "sns.scatterplot(x='installment',y='loan_amnt',data=df,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Relation between loan_amnt and loan_status ##\n",
    "sns.boxplot(x='loan_status',y='loan_amnt',data=df)\n",
    "\n",
    "df.groupby('loan_status')['loan_amnt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Showing the relation of loan_amnt with various grades and sub_grades ##\n",
    "sorted(df['grade'].unique())\n",
    "sorted(df['sub_grade'].unique())\n",
    "\n",
    "sns.countplot(x='grade',data=df,hue='loan_status')\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "subgrade_order = sorted(df['sub_grade'].unique())\n",
    "sns.countplot(x='sub_grade',data=df,order = subgrade_order,palette='coolwarm' ,hue='loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a new column called 'load_repaid' which will contain a 1 if the loan status was \"Fully Paid\" and a 0 if it was \"Charged Off\" ##\n",
    "df['loan_repaid'] = df['loan_status'].map({'Fully Paid':1,'Charged Off':0})\n",
    "df[['loan_repaid','loan_status']]\n",
    "\n",
    "## A bar plot showing the correlation of the numeric features to the new loan_repaid column ##\n",
    "df.corr()['loan_repaid'].sort_values().drop('loan_repaid').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding missing data ##\n",
    "df.isnull().sum()\n",
    "100* df.isnull().sum()/len(df)\n",
    "\n",
    "## Analysing emp_title ##\n",
    "df['emp_title'].value_counts()\n",
    "\n",
    "## Removing emp_title as a dummy variable ##\n",
    "df = df.drop('emp_title',axis=1)\n",
    "\n",
    "## Analysing emp_length ##\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.countplot(x='emp_length',data=df,order=emp_length_order,hue='loan_status')\n",
    "\n",
    "## Percentage of charge of per category ##\n",
    "emp_co = df[df['loan_status']==\"Charged Off\"].groupby(\"emp_length\").count()['loan_status']\n",
    "emp_fp = df[df['loan_status']==\"Fully Paid\"].groupby(\"emp_length\").count()['loan_status']\n",
    "emp_len = emp_co/emp_fp\n",
    "\n",
    "emp_len.plot(kind='bar')\n",
    "\n",
    "## Removing emp_length ##\n",
    "df = df.drop('emp_length',axis=1)\n",
    "\n",
    "## title and purpose are similar ##\n",
    "df['title'].head(10)\n",
    "df['purpose'].head(10)\n",
    "df = df.drop('title',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mortage_acc ##\n",
    "df.corr()['mort_acc'].sort_values()\n",
    "df.groupby('total_acc').mean()['mort_acc']\n",
    "\n",
    "total_acc_avg = df.groupby('total_acc').mean()['mort_acc']\n",
    "\n",
    "## fill in the missing mort_acc values based on their total_acc value ##\n",
    "\n",
    "def fill_mort_acc(total_acc,mort_acc):\n",
    "    '''\n",
    "    Accepts the total_acc and mort_acc values for the row.\n",
    "    Checks if the mort_acc is NaN , if so, it returns the avg mort_acc value\n",
    "    for the corresponding total_acc value for that row.\n",
    "    \n",
    "    total_acc_avg here should be a Series or dictionary containing the mapping of the\n",
    "    groupby averages of mort_acc per total_acc values.\n",
    "    '''\n",
    "    if np.isnan(mort_acc):\n",
    "        return total_acc_avg[total_acc]\n",
    "    else:\n",
    "        return mort_acc\n",
    "    \n",
    "df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)\n",
    "df.isnull().sum()\n",
    "\n",
    "## Removing the rows with missing value data ##\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorical and dummy variables ##\n",
    "df.select_dtypes(['object']).columns\n",
    "\n",
    "## Term to integer ##\n",
    "df['term'] = df['term'].apply(lambda term: int(term[:3]))\n",
    "\n",
    "## Keep just grade or sub_grade ##\n",
    "df = df.drop('grade',axis=1)\n",
    "\n",
    "## Converting sub_grades to dummy variables ##\n",
    "subgrade_dummies = pd.get_dummies(df['sub_grade'],drop_first=True)\n",
    "df = pd.concat([df.drop('sub_grade',axis=1),subgrade_dummies],axis=1)\n",
    "\n",
    "## Converting these columns: ['verification_status', 'application_type','initial_list_status','purpose'] into dummy variable ##\n",
    "dummies = pd.get_dummies(df[['verification_status', 'application_type','initial_list_status','purpose' ]],drop_first=True)\n",
    "df = df.drop(['verification_status', 'application_type','initial_list_status','purpose'],axis=1)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "## Home_owenership analysis ##\n",
    "df['home_ownership'].value_counts()\n",
    "\n",
    "df['home_ownership']=df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')\n",
    "\n",
    "dummies = pd.get_dummies(df['home_ownership'],drop_first=True)\n",
    "df = df.drop('home_ownership',axis=1)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "## New column Zip_code ##\n",
    "df['zip_code'] = df['address'].apply(lambda address:address[-5:])\n",
    "dummies = pd.get_dummies(df['zip_code'],drop_first=True)\n",
    "df = df.drop(['zip_code','address'],axis=1)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "df = df.drop('issue_d',axis=1)\n",
    "\n",
    "## Creating an new column 'earliest_cr_year' inclusing only year ##\n",
    "df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda date:int(date[-4:]))\n",
    "df = df.drop('earliest_cr_line',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split test and train data ##\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = df.drop('loan_status',axis=1)\n",
    "X = df.drop('loan_repaid',axis=1).values\n",
    "y = df['loan_repaid'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)\n",
    "\n",
    "## Normalizing the data ##\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Creation ##\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "## 78--> 39 --> 19 --> 1 ##\n",
    "model = Sequential()\n",
    "\n",
    "## Input layer ##\n",
    "model.add(Dense(78,  activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "## Hidden layer ##\n",
    "model.add(Dense(39, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "## Hidden layer ##\n",
    "model.add(Dense(19, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "## Output layer ##\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "## Compile model ##\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "## Training ##\n",
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=25,\n",
    "          batch_size=256,\n",
    "          validation_data=(X_test, y_test), \n",
    "          )\n",
    "\n",
    "## Save model ##\n",
    "from tensorflow.keras.models import load_model\n",
    "model.save('project_model.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot out the validation loss versus the training loss ##\n",
    "losses = pd.DataFrame(model.history.history)\n",
    "losses[['loss','val_loss']].plot()\n",
    "\n",
    "## Classification report ##\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "predictions = model.predict_classes(X_test)\n",
    "print(classification_report(y_test,predictions))\n",
    "confusion_matrix(y_test,predictions)\n",
    "\n",
    "## Offer a loan or not ##\n",
    "import random\n",
    "random.seed(101)\n",
    "random_ind = random.randint(0,len(df))\n",
    "\n",
    "new_customer = df.drop('loan_repaid',axis=1).iloc[random_ind]\n",
    "new_customer\n",
    "\n",
    "model.predict_classes(new_customer.values.reshape(1,78))\n",
    "df.iloc[random_ind]['loan_repaid']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
